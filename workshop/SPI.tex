\section{Strong parallelism invariance}
\label{sec:PI_causes}
In this section we describe potential violations of 
strong parallelism invariance (SPI) that can occur in a distributed setting. 
We also explain how we avoid these issues by using special 
distributed reduction schemes and splittable random number generators. 
Insights provided in this section can be applied 
to general distributed software beyond Julia.

 
We have identified two factors that can cause violations of our previously-defined 
SPI that standard Julia libraries 
do not automatically take care of:
\begin{enumerate}   
  \item \textbf{Non-associativity of floating point operations:} When 
  several workers perform distributed reduction of floating point values, 
  the output of this reduction will be slightly different depending on the order taken 
  during reduction. 
  When these reductions are then fed into further random operations, 
  this implies that two randomized algorithms with the same seed but using a 
  different number of workers will eventually arbitrarily diverge.

  \item \textbf{Global, thread-local, and task-local random number generators:} 
  These are the dominant approaches to parallel random number generators in current languages, 
  and an appropriate understanding of these RNGs is necessary. In particular, 
  in Julia it is important to understand the behaviour of the \texttt{@threads} macro.
\end{enumerate}
Our focus in the remainder of this section is to describe how our implementation 
solves the two above issues.


\subsection{Distributed reduction and floating point non-associativity}
After each round of PT, the machines need to exchange information such as the 
average swap acceptance probabilities, statistics to adapt a variational 
reference and adjust annealing parameters, and so on. 
For instance, suppose our \texttt{state} is univariate and real-valued and 
that each \texttt{Replica} keeps track of the number of times the target chain $N$ 
is visited as well as the mean of the univariate \texttt{state}s from the target chain.
To obtain the final estimate of the mean of the target distribution, we would like 
to pool the mean estimates from each of the \texttt{Replica}s, weighted by 
the number of times that each \texttt{Replica} visited the target chain.   
This process involves summing floating-point values that are located on each 
of the \texttt{Replica}s/machines and is an example of a reduction scheme.

 
To illustrate why distributed reduction with floating point values can violate 
strong parallelism invariance if not properly implemented, we consider the 
following toy example. 
Suppose we have 8 machines storing the floating point numbers $\cbra{1x, 2x, \ldots, 8x}$, 
as illustrated in \cref{fig:reduction_tree_8}, where we use $x = 10 e^1 \approx 27.1828$
in the following examples.
In this case, if our reduction procedure is to sum the floating point numbers, 
we know that our final answer should be approximately $36x$. 
However, depending on the exact order in which floating point addition is carried out, 
the answers might not all be the same and exactly equal to $36x$.
For instance, in \cref{fig:reduction_tree_8} we see that the order of operations 
for eight machines would be given by 
\[
  ((1x + 2x) + (3x + 4x)) + ((5x + 6x) + (7x + 8x)) \\
  \approx 978.5814582452562.
\]
In contrast, with two machines, one possible order of operations might be 
\[
  (((1x + 2x) + 3x) + 4x) + (((5x + 6x) + 7x) + 8x) \\
  \approx 978.5814582452563.
\]
A possible reduction tree for two machines is illustrated in \cref{fig:reduction_tree_2}.

 
To avoid the issue of non-associativity of 
floating point arithmetic, we ensure that the order in which operations are performed 
is exactly the same, irrespective of the number of processes/machines and threads. 
This is achieved by making sure that every value to be added---if addition is our 
reduction operation---is a leaf node in a reduction tree that is invariant to the 
number of machines available to perform the computation. 
For instance, if $N$ values are to be reduced, then the reduction tree would have 
$N$ leaf nodes. If $M$ machines are available, these machines are then assigned 
in such a way that the order of operations is as if there were $N$ machines available. 
To do so, it may be necessary for a machine to ``communicate with itself'', 
imitating the behaviour that would be present if there were $N$ machines available.
\cref{fig:reduction_tree_colour_8} and \cref{fig:reduction_tree_colour_2} 
illustrate the reduction procedure for eight and two machines, respectively.

\begin{figure}[t]
  \centering
  \begin{forest}
    for tree = {circle, draw, minimum size = 0.8cm}
      [{$36x$}
        [{$10x$}
          [{$3x$}
            [{$1x$}]
            [{$2x$}]
          ]
          [{$7x$}
            [{$3x$}] 
            [{$4x$}] 
          ]
        ]
        [{$26x$}
          [{$11x$}
            [{$5x$}] 
            [{$6x$}] 
          ]
          [{$15x$}
            [{$7x$}] 
            [{$8x$}] 
          ]
        ]
      ]
  \end{forest}
  \caption{Adding eight floating point numbers $\cbra{1x, 2x, \ldots, 8x}$ 
  across eight machines. 
  Additions in each row of the tree can be performed in parallel. 
  The final result is stored in the root node of the tree and can be represented 
  by the expression $((1x + 2x) + (3x + 4x)) + ((5x + 6x) + (7x + 8x))$, indicating 
  the order of operations.}
  \label{fig:reduction_tree_8}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{forest}
    for tree = {circle, draw, minimum size = 0.8cm}
      [{$36x$}
        [{$10x$}
          [{$6x$}
            [{$3x$}
              [{$1x$}]
              [{$2x$}]
            ]
            [{$3x$}]
          ]
          [{$4x$}]
        ]
        [{$26x$}
          [{$8x$}]
          [{$18x$}
            [{$7x$}]
            [{$11x$}
              [{$6x$}]
              [{$5x$}]
            ]
          ]
        ]
      ]
  \end{forest}
  \caption{One possible way of adding eight floating point numbers 
  $\cbra{1x, 2x, \ldots, 8x}$ across two machines. 
  The final result is stored in the root node of the tree and can be represented 
  by the expression $(((1x + 2x) + 3x) + 4x) + (((5x + 6x) + 7x) + 8x)$.
  Note that the order of operations in this expression is different from the one 
  presented in \cref{fig:reduction_tree_8}.}
  \label{fig:reduction_tree_2}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{forest}
    for tree = {circle, draw, minimum size = 0.8cm}
      [{$36x$}, fill = julia1
        [{$10x$}, fill = julia1
          [{$3x$}, fill = julia1
            [{$1x$}, fill = julia1]
            [{$2x$}, fill = julia2]
          ]
          [{$7x$}, fill = julia3
            [{$3x$}, fill = julia3] 
            [{$4x$}, fill = julia4] 
          ]
        ]
        [{$26x$}, fill = julia5
          [{$11x$}, fill = julia5
            [{$5x$}, fill = julia5] 
            [{$6x$}, fill = julia6] 
          ]
          [{$15x$}, fill = julia7
            [{$7x$}, fill = julia7] 
            [{$8x$}, fill = julia8] 
          ]
        ]
      ]
  \end{forest}
  \caption{Addition of eight floating point numbers across eight machines with a 
  guarantee on SPI. Each machine is represented by a different colour. 
  The final result can be represented 
  by the expression $((1x + 2x) + (3x + 4x)) + ((5x + 6x) + (7x + 8x))$.}
  \label{fig:reduction_tree_colour_8}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{forest}
    for tree = {circle, draw, minimum size = 0.8cm}
      [{$36x$}, fill = julia1
        [{$10x$}, fill = julia1
          [{$3x$}, fill = julia1
            [{$1x$}, fill = julia1]
            [{$2x$}, fill = julia1]
          ]
          [{$7x$}, fill = julia1
            [{$3x$}, fill = julia1] 
            [{$4x$}, fill = julia1] 
          ]
        ]
        [{$26x$}, fill = julia2
          [{$11x$}, fill = julia2
            [{$5x$}, fill = julia2] 
            [{$6x$}, fill = julia2] 
          ]
          [{$15x$}, fill = julia2
            [{$7x$}, fill = julia2] 
            [{$8x$}, fill = julia2] 
          ]
        ]
      ]
  \end{forest}
  \caption{Addition of eight floating point numbers across two machines with a 
  guarantee on SPI. Each machine is represented by a different colour. 
  The final result can be represented 
  by the expression $((1x + 2x) + (3x + 4x)) + ((5x + 6x) + (7x + 8x))$,
  which is the same as that given by eight machines in \cref{fig:reduction_tree_colour_8}.}
  \label{fig:reduction_tree_colour_2}
\end{figure}


\subsection{Splittable random streams}
\label{sec:splittable_randoms}
Another building block towards achieving SPI is a \emph{splittable random stream}
\cite{lecuyer1988splittable,burton1992splittable}. 
Julia uses \emph{task-local} random number generators, a notion that is 
related to (but does not necessarily imply) strong parallelism invariance. 
A \emph{task} is a unit of work on a machine.
A task-local RNG would then mean that a separate RNG is 
used for each unit of work, hopefully implying strong parallelism invariance
if the number of tasks is assumed to be constant. 
Unfortunately, this is not the case when a separate task is created for each thread 
of execution in Julia.
We note that the \texttt{@threads} macro in Julia creates \texttt{nthreads()}
tasks and thus \texttt{nthreads()} pseudorandom number generators.
This can break strong parallelism invariance as the output may depend on the number of 
threads.

 
To motivate splittable random streams, consider the following toy example 
that violates our notion of SPI:
\begin{lstlisting}[language = Julia]
using Random
import Base.Threads.@threads

println("Num. of threads: $(Threads.nthreads())")

const n_iters = 10000
result = zeros(n_iters)
Random.seed!(1)
@threads for i in 1:n_iters
    result[i] = rand()
end
println("Result: $(last(result))")
\end{lstlisting}
With eight threads, this outputs:
\begin{lstlisting}[language = Julia]
Num. of threads: 8
Result: 0.25679999169092793
\end{lstlisting}
Julia guarantees that if we rerun this code, as long as we are using eight threads, 
we will always get the same result, irrespective of the multi-threading 
scheduling decisions implied by the \texttt{@threads}-loop. 
Internally, Julia works with task-local RNGs and the \texttt{@threads} macro 
spawns \texttt{nthreads()} number of task-local RNGs.
For this reason, with a different number of threads, the result is different:
\begin{lstlisting}[language = Julia]
Num. of threads: 1
Result: 0.8785201210435906
\end{lstlisting}

In this simple example above, the difference in output is perhaps not too concerning, 
but for our parallel tempering 
use case, the distributed version of the algorithm is significantly more complex 
and difficult to debug compared to the single-threaded one. We therefore take task-local 
random number generation one step further and achieve SPI, which 
guarantees that the output is not only reproducible with respect to repetitions 
for a fixed number of threads, but also for different numbers of threads or processes.

 
A first step to achieve this is to associate one random number 
generator to each \texttt{Replica}. 
To do so, we use our SplittableRandoms.jl package, which is a Julia implementation 
of Java SplittableRandoms. Our package offers an implementation of  
SplitMix64 \cite{steele2014fast},
which allows us to turn one seed into an arbitrary collection of pseudo-independent 
RNGs. 
A quick example of how to use the SplittableRandoms.jl library is given below. 
By splitting a master RNG using the \texttt{split()} function, 
we can achieve SPI even with the use of the \texttt{@threads} macro. 

\begin{lstlisting}[language = Julia]
using Random
using SplittableRandoms: SplittableRandom, split
import Base.Threads.@threads

println("Num. of threads: $(Threads.nthreads())")

const n_iters = 10000
const master_rng = SplittableRandom(1)
result = zeros(n_iters)
rngs = [split(master_rng) for _ in 1:n_iters]
Random.seed!(1)
@threads for i in 1:n_iters
    result[i] = rand(rngs[i])
end
println("Result: $(last(result))")
\end{lstlisting}
With one and eight threads, the code above outputs
\begin{lstlisting}[language = Julia]
Num. of threads: 1
Result: 0.4394633333251359
\end{lstlisting}
\begin{lstlisting}[language = Julia]
Num. of threads: 8
Result: 0.4394633333251359
\end{lstlisting}