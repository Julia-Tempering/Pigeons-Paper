\section{Parallel tempering}
\label{sec:PT}
Pigeons.jl provides an implementation of distributed parallel tempering (PT) described 
in \cite{syed2021nrpt}, which we outline in Algorithm~\ref{alg:distributed_PT}.
This section gives both a brief overview of PT and some details of 
its distributed implementation.

\medskip 
In this section we assume a basic understanding of 
Markov chain Monte Carlo (MCMC) methods.
For readers unfamiliar with MCMC, it is important to know that 
it is a method to obtain approximate samples from a distribution. 
More specifically, it is a class of algorithms for simulating a Markov chain of 
states that look like draws from the target distribution $\pi$.
However, with traditional 
MCMC methods, the samples may be heavily correlated instead of independent 
and may fail to sufficiently explore the 
full space of the distribution (see \cref{fig:bimodal}); 
PT is a method that aims to address these issues.
For a more in-depth review of PT, we refer readers to \cite{surjanovic2022vpt}.


\subsection{Overview of PT}
Suppose that we would like to estimate integrals involving $\pi$, such as 
$\int f(x) \pi(x) \, \dee x$. 
These integrals may be multivariate and even include combinations of 
continuous and discrete variables (where sums replace integrals in the discrete 
case).
One method is to obtain samples from $\pi$ to approximate such integrals.
Often, the distribution 
$\pi$ can be challenging for traditional MCMC methods---such 
as Metropolis-Hastings, slice sampling, and Hamiltonian Monte Carlo---
because of its structure.
For example, in a bimodal example such as the one illustrated in 
Figure~\ref{fig:bimodal},
traditional methods might remain in one of the two modes for an extremely 
long period of time.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]{../img/bimodal.pdf}
    \caption{A simple bimodal distribution from which traditional 
    MCMC methods may struggle to obtain samples. Blue lines display output 
    from 1,000 iterations of a Metropolis-Hastings random walk MCMC algorithm.
    The sampler in this figure is visibly trapped in one of the two modes.}
    \label{fig:bimodal}
\end{figure}

To resolve this issue, PT constructs a sequence of $N$ distributions,  
$\pi_1, \pi_2, \ldots, \pi_N$, where $\pi_N$ is usually equal to $\pi$.
The distributions are chosen so that it is easy to obtain samples from $\pi_1$
with the sampling difficulty increasing as one approches $\pi_N$. An example 
of such a sequence of distributions, referred to as an \textit{annealing path},
is shown in Figure~\ref{fig:path}.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_1.pdf}
      \caption*{$\pi_1$}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_2.pdf}
      \caption*{$\pi_2$}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_3.pdf}
      \caption*{$\pi_3$}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_4.pdf}
      \caption*{$\pi_4$}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_5.pdf}
      \caption*{$\pi_5$}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/heatmap_path_6.pdf}
      \caption*{$\pi_6$}
    \end{minipage}
    \caption{Heatmaps of six distributions lying on an annealing path 
    from a unimodal reference distribution  
    $\pi_1$, from which it is straightforward to obtain samples, and ending at 
    $\pi_N$, which is in this case the bimodal distribution from 
    Figure~\ref{fig:bimodal}. (Note that in this case the colours between the heatmaps 
    are not directly comparable because the densities of intermediate distributions 
    are not normalized.)}
    \label{fig:path}
\end{figure}

We now turn to explain how this path of distributions can be used to enhance 
sampling from the target distribution, $\pi$. 
PT operates by first obtaining samples from each distribution on the path in parallel 
(referred to as an \textit{exploration phase}). 
Then, samples between adjacent distributions are swapped (referred to as 
a \textit{communication phase}). The communication phase in PT is crucial: it allows 
for the discovery of new regions of the space of the target distribution such as 
the top-right mode of the distribution presented in Figure~\ref{fig:bimodal}. 


\subsection{Local exploration and communication}
A struct that is useful for the implementation of PT is the \texttt{Replica}, 
which we will often refer to in the remainder of this section. 
A single \texttt{Replica} struct stores a \texttt{state} variable and a \texttt{chain} 
integer, among other entries. 
At the beginning of PT, $N$ \texttt{Replicas} are created, 
one for each distribution on the annealing path, and 
the chain entries in the $N$ replicas are initialized at $1,2,\ldots,N$, respectively. 
For a given \texttt{Replica}, if the \texttt{chain} number is $i$ and the \texttt{state}
is $x$, then this means that the sample corresponding to the $i$-th distribution in 
the sequence $\pi_1, \ldots, \pi_N$ is currently at location $x$.  

\medskip 
In the local exploration phase,
each \texttt{Replica}'s state is modified using an MCMC move targeting $\pi_i$,
where $i$ is given by \texttt{Replica.chain}.
The MCMC move can either modify \texttt{Replica.state} in-place, or modify the 
\texttt{Replica}'s \texttt{state} field. 
This operation is indicated by the \texttt{local\_exploration} function in 
\cref{alg:distributed_PT}.

\medskip 
In the communication phase, PT proposes swaps between pairs of replicas. 
In principle, there are two equivalent ways to do a swap.
In the first implementation, the \texttt{Replica}s 
could exchange their \texttt{state} fields.
Alternatively, they could exchange their \texttt{chain} fields.
Because we provide distributed implementations, we use the latter as it ensures that 
the amount of data that needs to be exchanged between two machines during a swap 
can be made very small (two floating point numbers), resuling in 
an exchange of $O(N)$ messages of size $O(1)$. 
Note that this cost does not vary with the dimensionality of the state space, 
in contrast to the first implementation that would transmit 
$O(N)$ messages of size $O(d)$, where $d$ is the dimension of the state space,
incurring a potentially very high communication cost for large values of $d$.


\subsection{Distributed implementation}
A distributed implementation of PT is presented in \cref{alg:distributed_PT}
and we describe the details of the implementation below.
We present the algorithm from the perspective that the number of machines available 
is equal to the number of \texttt{Replica}s and distributions, $N$. 
However, Pigeons.jl also allows for the more general case where the number of 
machines is not necessarily equal to $N$.

\subsubsection{The \texttt{PermutedDistributedArray}}
\label{sec:permuted_dist_array}
Recall that our theoretical $O(1)$ message size for communication between two machines 
is achieved by exchanging \texttt{chain} indices between \texttt{Replica}s 
instead of their \texttt{state}s.
One difficulty can be encountered in a distributed implementation, which we 
illustrate with the following simple example. 
Suppose we have $N=4$ distributions, chains, replicas, and machines, and that 
machine 1 is exploring chain 2 while machine 4 is exploring chain 3.
At one point, the \texttt{Replica} at \texttt{chain} 2 (machine 1) might need to exchange
\texttt{chain} indices with the \texttt{Replica} that has \texttt{chain} index 3 (machine 4). 
However, a priori it is not clear how machine 1 should know that it should communicate 
with machine 4 because it has no knowledge about the chain indices on the other machines.

\medskip 
To resolve this issue, we introduce a special data structure called a 
\texttt{PermutedDistributedArray}. In the case where the number of replicas is 
equal to the number of available machines, the construction is quite simple
and effectively results in each machine storing one additional integer. 
Considering the same example above, the solution to the problem is to have
machine 1 (\texttt{chain} 2 wanting to communicate with \texttt{chain} 3) 
communicate with \textit{machine} 3. 
Machine 3 stores in its \texttt{dist\_array} variable of type 
\texttt{PermutedDistributedArray} the value 4, which is the machine number that 
stores \texttt{chain} 3. By updating these \texttt{PermutedDistributedArray} 
variables at each communication step, we can ensure that each machine $j$ is aware 
of which machine number currently stores \texttt{chain} $j$. 
Therefore, the machines act as keys in a dictionary for the communication 
permutations.
An illustration of communication between four machines is provided in 
\cref{fig:index_process}.
We note that with a \texttt{PermutedDistributedArray} we make special assumptions 
on how we access and write to the array elements. 
Several MPI processes cooperate, with each machine storing 
data for a slice of this distributed array, and at each time 
step an index of the array is manipulated by exactly one machine.
% With these assumptions we achieve read/write costs that are 
% near-constant in the number of machines participating. 

\begin{figure*}[t] 
  \centering 
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.39\textwidth]{../img/Communication_1_v3.pdf}
    \includegraphics[width=0.59\textwidth]{../img/index_process_1.pdf}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.39\textwidth]{../img/Communication_2_v3.pdf}
    \includegraphics[width=0.59\textwidth]{../img/index_process_2.pdf}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.39\textwidth]{../img/Communication_3_v3.pdf}
    \includegraphics[width=0.59\textwidth]{../img/index_process_3.pdf}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.39\textwidth]{../img/Communication_4_v3.pdf}
    \includegraphics[width=0.59\textwidth]{../img/index_process_4.pdf}
  \end{minipage}
  \caption{
    A summary of communication between four machines with $N=4$ PT chains.
    Circles represent machines with the machine number in the center of the circle. 
    Different colours are also used for different machines.
    Numbers in parantheses represent the \texttt{chain} 
    that is currently being explored by that machine. 
    Arrows indicate communication occuring between machines to exchange \texttt{chain} 
    indices.
    To keep track of which chains are stored on which machine, we introduce the 
    \texttt{dist\_array} of type \texttt{PermutedDistributedArray}, described in 
    \cref{sec:permuted_dist_array}, which is of length $N$. 
    The $j$-th element of \texttt{dist\_array} at a given time step indicates which 
    machine number is storing \texttt{chain} index $j$.
    The bolded curve in the figures to the right indicate the trajectory of the first 
    \texttt{Replica} over the course of each of the communication steps.      
    \textbf{Top left:} the PT replicas are initialized and the first communication step 
    is proposed. At this time step, $\texttt{dist\_array} = [1,2,3,4]$.
    \textbf{Top right:} after the first successful communication step, we now have 
    $\texttt{dist\_array} = [2,1,4,3]$.
    \textbf{Bottom left:} The second communication step is completed and 
    $\texttt{dist\_array} = [2,4,1,3]$.
    \textbf{Bottom right:} The third communication step is completed and 
    $\texttt{dist\_array} = [4,2,3,1]$.}
  \label{fig:index_process}
\end{figure*}

\subsubsection{MPI implementation details}
We use the MPI.jl \cite{byrne2021mpi} package to support communication between machines. 
In our pseudocode in \cref{alg:distributed_PT} we define the MPI-style 
functions \texttt{send()}, \texttt{receive!()}, 
and \texttt{waitall()}. 
The function \texttt{send()} has three arguments in our pseudocode: the first argument 
is the object to be sent, the second is the machine number to which the object should 
be sent, and finally the third argument is a unique tag for this specific send/receive 
request at this time step and on this machine, generated by \texttt{tag()}.
The \texttt{tag()} function can be any function that allows one to uniquely identify 
a tag for a given send/receive request on a given machine at any given time step $t$.
The function \texttt{receive!()} has the same last two arguments as \texttt{send()} 
except that the first argument 
specifies the object to which data should be written directly. Finally, 
\texttt{waitall(requests)} waits for all initialized MPI \texttt{requests} for a given pair 
of machines to complete.

\medskip 
In \cref{alg:distributed_PT} we also introduce the functions 
\texttt{permuted\_get()} and \texttt{permuted\_set!()}. 
Given the current machine's \texttt{dist\_array} and the 
\texttt{partner} chain with which it should communicate, 
\texttt{permuted\_get()} returns the machine number that holds the \texttt{partner} chain. 
Given the current machine's \texttt{dist\_array}, \texttt{chain} number, and machine number $j$,
\texttt{permuted\_set!()} updates the \texttt{PermutedDistributedArray} variables 
with the updated permutation.

\begin{algorithm}[t]
	\begin{algorithmic}[1]
    \Require Initial state $x_0$, sequence of distributions $\{\pi_i\}_{i=1}^N$, 
      number of iterations $T$, machine number $j$
    \State $\texttt{chain} \gets j$ \Comment{current \texttt{chain} number}
    \State $\texttt{dist\_array}[j] \gets j$ \Comment{\texttt{chain} $j$ is on machine $j$}
		
    \For{$t$ {\bf in} 1, 2, \dots, $T$}
		  \If{$t$ is even} \Comment{choose between even or odd swap}
		    \State $P \gets \{i: 1 \le i < N, i \text{ is even} \}$
		  \Else
		    \State $P \gets \{i: 1 \le i < N, i \text{ is odd} \}$
		  \EndIf

      \State $x_t \gets \texttt{local\_exploration}(\pi_\texttt{chain}, x_{t-1})$
      \State $\texttt{partner} \gets \texttt{nothing}$
      \If{$\texttt{chain} \in P$}
        \State $\texttt{partner} \gets \texttt{chain}+1$
      \ElsIf{$\texttt{chain}-1 \in P$}
        \State $\texttt{partner} \gets \texttt{chain}-1$
      \EndIf
      \If{$\texttt{partner != nothing}$ \textbf{and} \text{swap is accepted}}
        \State $\texttt{to\_machine} \gets \texttt{permuted\_get}(\texttt{dist\_array}, \texttt{partner})$
          \Comment{retrieve \texttt{dist\_array}[\texttt{partner}]}
        \State $\texttt{send\_tag} \gets \texttt{tag}(t, \texttt{chain}, j)$
        \State $\texttt{send}(\texttt{chain}, \texttt{to\_machine}, \texttt{send\_tag})$
        \State $\texttt{receive\_tag} \gets \texttt{tag}(t, \texttt{chain}, \texttt{to\_machine})$
        \State $\texttt{receive!}(\texttt{chain}, \texttt{to\_machine}, \texttt{receive\_tag})$
        \State $\texttt{waitall(requests)}$ 
          \Comment{wait for MPI requests for this pair of machines to complete}
        \State $\texttt{permuted\_set!}(\texttt{dist\_array}, \texttt{chain}, j)$
          \Comment{set value of \texttt{dist\_array}[\texttt{chain}] to \texttt{to\_machine}}
      \EndIf
      \State $c_t \gets \texttt{chain}$ 
		\EndFor
    \State \Return $\{(x_t, c_t)\}_{t=1}^T$
	\end{algorithmic}
  \caption{Distributed PT on machine $j$ (one replica per machine)}
  \label{alg:distributed_PT}
\end{algorithm}


\subsection{Strong and weak scaling of PT}

We study the strong and weak scaling properties of Pigeons' implementation of PT
as given by Amdahl's law and Gustafson's law, respectively.
For the strong scaling results, we consider a fixed problem size and compare the 
compute time as we vary the number of processes.  
For weak scaling, we consider a varying problem size proportional 
to the number of processes used. 
In both cases we use the number of chains as the definition of the problem size; 
this is in line with previous heuristics that establish that the generaly problem 
difficulty is proportional to the optimal number of chains in PT \cite{syed2021nrpt}.

\medskip
We emphasize again that Pigeons supports both shared memory 
parallelism (SMP) and distributed memory parallelism (DMP). However, in certain cases, it 
might be useful to run parallel tempering with very many chains or 
to run several instances of parallel tempering at once. In such cases, 
the DMP setup is the most natural. Our implementation in Pigeons ensures that the 
communication cost in this setup is minimal. In most cases, the 
communication cost is negligible compared to other computations performed on each process. 

\medskip
The results for strong and weak scaling are presented in \cref{fig:PT_scaling}. 
For strong scaling, we see that ...
For weak scaling, We see that ...
\NS{Comment on plots}.

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/trace_density_plots_pt.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../img/trace_density_plots_single_chain.pdf}
    \end{minipage}
    \caption{
        \NS{Add appropriate plots.}
        Strong and weak scaling results for PT.  
        \textbf{Left:} Strong scaling. 
        \textbf{Right:} Weak scaling. 
    }
    \label{fig:PT_scaling}
\end{figure*}

